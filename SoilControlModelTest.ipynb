{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUhvVVe1eZIr"
      },
      "source": [
        "\n",
        "\n",
        "*   https://it-start.online/articles/grafik-obnovljaemyj-v-realnom-vremeni-na-python - движущийся обновляющийся график\n",
        "*   https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html - DQN\n",
        "* https://habr.com/ru/articles/719544/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6AvdpSD1ezG",
        "outputId": "1c57c16a-840c-4f03-dc8a-c29fcb7ea049"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip install --upgrade pip\n",
        "!pip install gymnasium[classic_control] Cython  tensorflow  gym keras  keras-rl2   pyvirtualdisplay  pyglet\n",
        "!pip install matplotlib\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2cvvBsCeSL_"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Flatten\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from keras import __version__\n",
        "tf.keras.__version__ = __version__\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.random import OrnsteinUhlenbeckProcess\n",
        "from rl.agents import DDPGAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-TRbipx8i1p"
      },
      "source": [
        "Используя датчик 7 в 1 для измерения показателей субстрата (земли с добавками), который будет использоваться в гидропонных системах выращивания. Сможем получать информацию от каждого растения:\n",
        "* температура\n",
        "* влажность субстрата;\n",
        "* pH - кислотность;\n",
        "* EC/TDS - общая минерализация;\n",
        "* NPK количество\n",
        "  * N азота\n",
        "  * P фосфора\n",
        "  * K калия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89dR4zX4-EZM"
      },
      "source": [
        "Показатели почвы, которые влияют на рост и развитие картофеля, включают следующие:\n",
        "\n",
        "1. Температура:\n",
        "   - Оптимальная температура почвы для картофеля составляет примерно 13-16 градусов Цельсия.\n",
        "   - Рост картофеля замедляется при температуре ниже 10 градусов Цельсия и прекращается при температуре ниже 5 градусов Цельсия.\n",
        "\n",
        "2. Влажность:\n",
        "   - Картофель требует умеренной влажности почвы для хорошего роста и развития.\n",
        "   - Оптимальный уровень влажности почвы для картофеля составляет примерно 60-70% от влагоемкости почвы.\n",
        "   - Недостаток влаги может привести к медленному росту и низкому урожаю, а избыток влаги может способствовать развитию гнили и заболеваний.\n",
        "\n",
        "3. pH кислотность:\n",
        "   - Почва для картофеля должна иметь pH в диапазоне 5.0-6.5.\n",
        "   - Низкое pH подавляет доступность необходимых питательных веществ для растения, а высокое pH может привести к блокировке определенных элементов питания.\n",
        "\n",
        "4. EC/TDS - общая минерализация:\n",
        "   - EC (Electrical Conductivity) или TDS (Total Dissolved Solids) используется для измерения общей минерализации почвы.\n",
        "   - Общая минерализация должна находиться в допустимых пределах, которые могут варьироваться в зависимости от вида почвы и требований картофеля.\n",
        "   - Оптимальные значения общей минерализации (EC/TDS) в субстрате для выращивания картофеля могут варьироваться в зависимости от различных факторов, таких как тип почвы, климатические условия, сорт картофеля и метод выращивания. Обычно принимаются следующие рекомендации по значениям EC/TDS:\n",
        "\n",
        "    - *Для начала выращивания картофеля*: рекомендуется значение EC/TDS примерно **1,2-1,5 мСм/см** (декасименс на метр, мкСм/см), что соответствует умеренной минерализации.\n",
        "\n",
        "    - *Во время роста растений*: рекомендуется значение EC/TDS примерно **1,5-2 мСм/см** для поддержания оптимальных условий роста и развития картофеля.\n",
        "\n",
        "    - *Перед сбором урожая*: рекомендуется снизить значение EC/TDS до около **0,8-1,2 мСм/см** для достижения лучшего качества и вкуса картофеля.\n",
        "\n",
        "    - Важно отметить, что эти значения служат только ориентиром, и оптимальные показатели могут незначительно отличаться в зависимости от местных условий и предпочтений роста. Рекомендуется проведение агрохимического анализа почвы и консультация с агрономом или специалистом по грунту для определения конкретных требований и практик, которые следует применять при выращивании картофеля в вашем регионе.\n",
        "\n",
        "5. NPK (азот, фосфор, калий):\n",
        "   - Картофель требует определенного соотношения азота (N), фосфора (P) и калия (K) для нормального роста и развития.\n",
        "   - Рекомендуемые значения NPK зависят от состояния почвы и могут быть получены через агрохимический анализ.\n",
        "   - NPK относится к макрэлементам, которые являются основными питательными веществами для роста и развития растений. Они представлены следующими элементами: азот (N), фосфор (P) и калий (K). Для выращивания картофеля рекомендуется следующие NPK показатели:\n",
        "\n",
        "    * Азот *(N)*: Азот необходим для развития листвы, стеблей и общей зеленой массы растения. Рекомендуется N-показатель примерно **120-150 кг/га** для картофеля в начале сезона и **60-90 кг/га** во время активного роста.\n",
        "\n",
        "    * Фосфор *(P)*: Фосфор необходим для развития корней, цветения и формирования клубней. Рекомендуется P-показатель примерно **60-90 кг/га** для картофеля в начале сезона и **120-150 кг/га** во время активного роста.\n",
        "\n",
        "    * Калий *(K)*: Калий необходим для образования и развития клубней, а также повышения стойкости растений к стрессу и болезням. Рекомендуется K-показатель примерно **120-150 кг/га** для картофеля в начале сезона и **150-180 кг/га** во время активного роста.\n",
        "\n",
        "   - Эти значения являются общими рекомендациями и могут варьироваться в зависимости от типа почвы, условий выращивания и сорта картофеля. Рекомендуется проведение агрохимического анализа почвы и консультация с агрономом или специалистом по грунту для определения конкретных потребностей в NPK и разработки индивидуального плана питания для выращивания картофеля.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh9bzXyrxAOX"
      },
      "outputs": [],
      "source": [
        "def NRandom(n):\n",
        "    # Генерируем первые два числа\n",
        "    numbs = [\n",
        "        random.uniform(0,1) for _ in range(n-1)\n",
        "    ]\n",
        "    numbs.append(1 - sum(numbs))\n",
        "    if all([0 < x < 1 for x in numbs]):\n",
        "      return np.array(numbs)\n",
        "    else:\n",
        "      return NRandom(n)\n",
        "\n",
        "# Коэффициент теплопроводности (k): Коэффициент теплопроводности характеризует способность почвы проводить тепло. Для чернозема обычно принимают значение около 0.25 Вт/(м·К) при нормальных условиях (сухой почве и определенной температуре).\n",
        "\n",
        "# Коэффициент проводимости влаги (K): Коэффициент проводимости влаги определяет, насколько быстро вода проникает в почву. Значение K также будет зависеть от многих факторов, но для чернозема оно может варьироваться от 0.05 до 0.5 см/час."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq4OIl1kjPFT"
      },
      "outputs": [],
      "source": [
        "class SoilSystemEnv(gym.Env):\n",
        "  def __init__(\n",
        "      self,\n",
        "      optimal_space:dict,#{\"Space_Var\":[opt_start=-np.inf,opt_end=np.inf]},\n",
        "      V:float=1, # объем горшка,\n",
        "      alpha:float=0.2, # для расчета температуры почвы: T_soil` = T_air - alpha(T_air - T_soil) alpha in [0.1 0.3]\n",
        "      beta:float=0.3, # для расчета влажности почвы: зрш` = phi_soil` + beta(phi_air - phi_soil)*100 alpha in [0.1 0.5]\n",
        "      random_state:int=42,\n",
        "      eps:float=1e-5,\n",
        "      k:float=1e-3\n",
        "      ):\n",
        "        # Actions we can take, down, stay, up\n",
        "        #  В качестве действия агент(ы) могут подкручивать три винтиля (состава растворов №1 и №2, количество добавляемой воды до 1 литра) +\n",
        "        self.action_space = spaces.Box(\n",
        "          low=np.array([  -1,  -1,  -1, -1,  -1]),\n",
        "          high=np.array([ 1,  1,  1,  1,  1]), # Valve_NPK; Valve_pH|_EC_TDS; Valve_Water; T_air; phi_air\n",
        "          shape=(5,),dtype=float) \n",
        "        self.observation_space = spaces.Box(\n",
        "          low=\n",
        "          np.array([0,  0,  0,    40*1e-5, 40*1e-5, 40*1e-5]), # T_soil phi_soil pH_soil EC_TDS N P K\n",
        "          high=\n",
        "          np.array([35,  1,  1,  200*1e-5,  200*1e-5,  200*1e-5]),\n",
        "          shape=(6,),dtype=float)\n",
        "\n",
        "        self.optimal_space = {\n",
        "                # Soil\n",
        "                'T_soil': np.array([optimal_space['T_soil'][0],optimal_space['T_soil'][1]]),\n",
        "                'phi_soil': np.array([optimal_space['phi_soil'][0],optimal_space['phi_soil'][1]]),\n",
        "                'pH_soil': np.array([optimal_space['pH_soil'][0],optimal_space['pH_soil'][1]]),\n",
        "                # 'EC_TDS': np.array([optimal_space['EC_TDS'][0],optimal_space['EC_TDS'][1]]),\n",
        "                'N': np.array([optimal_space['N'][0],optimal_space['N'][1]]),\n",
        "                'P': np.array([optimal_space['P'][0],optimal_space['P'][1]]),\n",
        "                'K': np.array([optimal_space['K'][0],optimal_space['K'][1]]),\n",
        "        }\n",
        "        self.state = self.observation_space.sample() \n",
        "        self.V = V\n",
        "        self.k=k\n",
        "\n",
        "        # Valve 1 params (random)\n",
        "        self.valve_1_N = np.array([np.mean(self.optimal_space[param]) - np.min(np.concatenate([self.optimal_space[par] for  par in ['N','P','K']])) for param in ['N','P','K']]) / \\\n",
        "          (np.max(np.concatenate([self.optimal_space[par] for  par in ['N','P','K']])) - np.min(np.concatenate([self.optimal_space[par] for  par in ['N','P','K']]))) # N, P, K\n",
        "\n",
        "        # Soil propereties\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "        print(\n",
        "            f'valve_1_N: {self.valve_1_N}'+\n",
        "            # f'\\nvalve_2_N: {self.valve_2_N}'+\n",
        "            f'\\nalpha: {self.alpha}'+\n",
        "            f'\\nbeta: {self.beta}'+\n",
        "            \n",
        "            f'\\nSTATE_START:'+\n",
        "            f'\\n T_soil: {self.state[0]}'+#{self.state[[\"T_soil\"]]}'+\n",
        "            f'\\n phi_soil: {self.state[1]}'+#{self.state[\"phi_soil\"]}'+\n",
        "            f'\\n pH_soil: {self.state[2]}'+#{self.state[\"pH_soil\"]}'+\n",
        "            # f'\\n EC_TDS: {self.state[3]}'+#{self.state[\"EC_TDS\"]}'+\n",
        "            f'\\n [N P K]: [{self.state[3]} {self.state[4]} {self.state[5]}]'#{self.state[\"N\"]} {self.state[\"P\"]} {self.state[\"K\"]}]'+\n",
        "            \n",
        "            f'\\nSTATE_OPTIMAL:'+\n",
        "            f'\\n T_soil: {self.optimal_space[\"T_soil\"]}'+\n",
        "            f'\\n phi_soil: {self.optimal_space[\"phi_soil\"]}'+\n",
        "            f'\\n pH_soil: {self.optimal_space[\"pH_soil\"]}'+\n",
        "            # f'\\n EC_TDS: {self.optimal_space[\"EC_TDS\"]}'+\n",
        "            f'\\n [N P K]: [{self.optimal_space[\"N\"]} {self.optimal_space[\"P\"]} {self.optimal_space[\"K\"]}]'\n",
        "              )\n",
        "\n",
        "  def step(self,action):\n",
        "        # 1. Apply action\n",
        "        #№  Get new params\n",
        "        T_air = 18 + action[-2] * 12\n",
        "        water = action[2] * self.V\n",
        "        \n",
        "        phi_air = action[-1]\n",
        "        \n",
        "        N_act, P_act, K_act = action[0] * self.valve_1_N * water\n",
        "        pH_soil = action[1]#, EC_TDS = action[1] * self.valve_2_N*water\n",
        "        # pH_soil = action[1]  \n",
        "\n",
        "        # 2. Apply State\n",
        "        ##  T_soil\n",
        "        self.state[0] =  T_air - self.alpha * (T_air - self.state[0])\n",
        "        ##  phi_soil\n",
        "        self.state[1] =  phi_air + self.beta * (phi_air - self.state[1])\n",
        "        self.state[1] = self.state[1] + water/self.V if self.state[1] + water/self.V < 1 else 1\n",
        "        ## NPK\n",
        "        self.state[3] += N_act + random.uniform(-.10,0)\n",
        "        self.state[4] += P_act + random.uniform(-.10,0)\n",
        "        self.state[5] += K_act + random.uniform(-.10,0)\n",
        "        ## pH_soil, EC_TDS\n",
        "        self.state[2] += pH_soil + random.uniform(-.10,0)\n",
        "        # self.state[3] += EC_TDS + random.uniform(-1.0,0)\n",
        "\n",
        "        # 3. Get reward\n",
        "        # Вычисление вознаграждения\n",
        "        reward = self._get_reward(self.state)\n",
        "\n",
        "        # Определение, является ли эпизод завершенным\n",
        "        if abs(reward - 1) <=self.eps:\n",
        "          \n",
        "          done=True\n",
        "        else:\n",
        "          done = False\n",
        "        # print(f'self.reward={reward}')\n",
        "        # Set placeholder for info\n",
        "        info = {}\n",
        "\n",
        "        # Return step information\n",
        "        return self.state, reward, done, info\n",
        "  def render(self):\n",
        "    # Визуализация текущего состояния среды\n",
        "    pass\n",
        "  def _get_reward(self, state):     \n",
        "      keys = ['T_soil','phi_soil','pH_soil','N','P','K']\n",
        "      # reward_list = np.array([1 if self.optimal_space[keys[i]][0] <= state[i] <= self.optimal_space[keys[i]][1] else np.clip(np.exp(np.mean(self.optimal_space[keys[i]]) - state[i]),0,1) for i in range(len(keys))])  \n",
        "      # reward_list = np.array([1 if self.optimal_space[keys[i]][0] <= state[i] <= self.optimal_space[keys[i]][1] else 0 for i in range(len(keys))])    \n",
        "      clipped_reward = np.clip(np.mean(reward_list), 0, 1)\n",
        "      return clipped_reward\n",
        "\n",
        "  def reset(self):\n",
        "    self.state =self.observation_space.sample()\n",
        "    return self.state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opr57jQl3Zem"
      },
      "outputs": [],
      "source": [
        "# # 1. Дискретные пространства состояний: Дискретные пространства состояний представляются целыми числами или наборами дискретных значений. Например, игры с сеткой, где состояния могут иметь конечное количество дискретных позиций, могут использовать дискретные пространства состояний.\n",
        "\n",
        "# #    Примеры дискретных пространств состояний в Gym:\n",
        "# #    - `spaces.Discrete(n)`: пространство из `n` дискретных состояний.\n",
        "# #    - `spaces.MultiDiscrete(nvec)`: многомерное пространство состояний с `nvec` дискретными значениями по каждому измерению.\n",
        "\n",
        "# # 2. Непрерывные пространства состояний: Непрерывные пространства состояний представляются вещественными числами или интервалами значений. Например, в задачах управления роботами или автономными автомобилями, где состояния могут быть описаны непрерывными переменными, можно использовать непрерывные пространства состояний.\n",
        "\n",
        "# #    Примеры непрерывных пространств состояний в Gym:\n",
        "# #    - `spaces.Box(low, high, shape)`: пространство состояний, представленное в виде прямоугольника с нижними (`low`) и верхними (`high`) границами значений переменных в каждом измерении.\n",
        "# #    - `spaces.MultiBox([spaces.Box(), spaces.Box(), ...])`: многомерное пространство состояний, состоящее из нескольких непрерывных пространств состояний.\n",
        "# # Например, если начальная температура почвы T0 и окружающая среда имеет постоянную температуру T_env, то решение может иметь вид:\n",
        "\n",
        "# # T(t) = T_env + (T0 - T_env) * exp(-kt)\n",
        "\n",
        "# # где t - время.\n",
        "\n",
        "# # В данном случае, значение коэффициента охлаждения k будет определяться конкретными условиями (материал горшка, его размеры и т.д.), а начальная температура почвы и температура окружающей среды будут параметрами, которые следует учесть в оценке остывания почвы.\n",
        "\n",
        "# class SoilSystemEnv(gym.Env):\n",
        "#   def __init__(\n",
        "#       self,\n",
        "#       optimal_space:dict,#{\"Space_Var\":[opt_start=-np.inf,opt_end=np.inf]},\n",
        "#       V:float=1, # объем горшка,\n",
        "#       alpha:float=0.2, # для расчета температуры почвы: T_soil` = T_air - alpha(T_air - T_soil) alpha in [0.1 0.3]\n",
        "#       beta:float=0.3, # для расчета влажности почвы: зрш` = phi_soil` + beta(phi_air - phi_soil)*100 alpha in [0.1 0.5]\n",
        "#       random_state:int=42\n",
        "#       ):\n",
        "#         # Actions we can take, down, stay, up\n",
        "#         #  В качестве действия агент(ы) могут подкручивать три винтиля (состава растворов №1 и №2, количество добавляемой воды до 1 литра) +\n",
        "#         self.action_space = spaces.Box(low=np.array([0 for _ in range(5)]), high=np.array([1 for _ in range(5)]),shape=(5,),dtype=float)\n",
        "#             # Valve_NPK; Valve_pH_EC_TDS; Valve_Water; T_air; phi_air\n",
        "#         self.observation_space = spaces.Box(low=np.array([-100,0,0,0,0,0,0]),high=np.array([100,1,14,np.inf,np.inf,np.inf,np.inf]),shape=(7,),dtype=float)\n",
        "#         #         'T_soil': random.uniform(-100, 100),\n",
        "#         #         'phi_soil': random.uniform(0, 1),\n",
        "#         #         'pH_soil': random.uniform(0, 14),\n",
        "#         #         'EC_TDS': random.uniform(0.2, 5),\n",
        "#         #         'N': random.uniform(0, 250),\n",
        "#         #         'P': random.uniform(0, 250),\n",
        "#         #         'K': random.uniform(0, 250)\n",
        "\n",
        "#         self.optimal_space = {\n",
        "#                 # Soil\n",
        "#                 'T_soil': np.array([optimal_space['T_soil'][0],optimal_space['T_soil'][1]]),\n",
        "#                 'phi_soil': np.array([optimal_space['phi_soil'][0],optimal_space['phi_soil'][1]]),\n",
        "#                 'pH_soil': np.array([optimal_space['pH_soil'][0],optimal_space['pH_soil'][1]]),\n",
        "#                 'EC_TDS': np.array([optimal_space['EC_TDS'][0],optimal_space['EC_TDS'][1]]),\n",
        "#                 'N': np.array([optimal_space['N'][0],optimal_space['N'][1]]),\n",
        "#                 'P': np.array([optimal_space['P'][0],optimal_space['P'][1]]),\n",
        "#                 'K': np.array([optimal_space['K'][0],optimal_space['K'][1]]),\n",
        "#         }\n",
        "#         # Set start temp\n",
        "#         self.state = self.observation_space.sample() #{\n",
        "#         #         'T_soil': random.uniform(-100, 100),\n",
        "#         #         'phi_soil': random.uniform(0, 1),\n",
        "#         #         'pH_soil': random.uniform(0, 14),\n",
        "#         #         'EC_TDS': random.uniform(0.2, 5),\n",
        "#         #         'N': random.uniform(0, 250),\n",
        "#         #         'P': random.uniform(0, 250),\n",
        "#         #         'K': random.uniform(0, 250)\n",
        "\n",
        "#         # }\n",
        "#         # V - объем\n",
        "#         self.V = V\n",
        "\n",
        "#         # Valve 1 params (random)\n",
        "#         self.valve_1_N = NRandom(3) # N, P, K\n",
        "#         # Valve 2 params (random)\n",
        "#         self.valve_2_N = NRandom(2) # pH_soil, EC_TDS\n",
        "\n",
        "#         # Soil propereties\n",
        "#         self.alpha = alpha\n",
        "#         self.beta = beta\n",
        "#         print(\n",
        "#             f'valve_1_N: {self.valve_1_N}'+\n",
        "#             f'\\nvalve_2_N: {self.valve_2_N}'+\n",
        "#             f'\\nalpha: {self.alpha}'+\n",
        "#             f'\\nbeta: {self.beta}'+\n",
        "#             f'\\nSTATE_START:'+\n",
        "#             f'\\n T_soil: {self.state[0]}'+#{self.state[[\"T_soil\"]]}'+\n",
        "#             f'\\n phi_soil: {self.state[1]}'+#{self.state[\"phi_soil\"]}'+\n",
        "#             f'\\n pH_soil: {self.state[2]}'+#{self.state[\"pH_soil\"]}'+\n",
        "#             f'\\n EC_TDS: {self.state[3]}'+#{self.state[\"EC_TDS\"]}'+\n",
        "#             f'\\n [N P K]: [{self.state[4]} {self.state[5]} {self.state[6]}]'#{self.state[\"N\"]} {self.state[\"P\"]} {self.state[\"K\"]}]'+\n",
        "#             f'\\nSTATE_OPTIMAL:'+\n",
        "#             f'\\n T_soil: {self.optimal_space[\"T_soil\"]}'+\n",
        "#             f'\\n phi_soil: {self.optimal_space[\"phi_soil\"]}'+\n",
        "#             f'\\n pH_soil: {self.optimal_space[\"pH_soil\"]}'+\n",
        "#             f'\\n EC_TDS: {self.optimal_space[\"EC_TDS\"]}'+\n",
        "#             f'\\n [N P K]: [{self.optimal_space[\"N\"]} {self.optimal_space[\"P\"]} {self.optimal_space[\"K\"]}]'\n",
        "#               )\n",
        "\n",
        "#   def step(self,action):\n",
        "#         # 1. Apply action\n",
        "#         #№  Get new params\n",
        "#         T_air = 18 + action[-2] * 12\n",
        "#         phi_air = action[-1]\n",
        "#         N_act,P_act,K_act = action[0] * self.valve_1_N\n",
        "#         pH_soil, EC_TDS = action[1] * self.valve_2_N\n",
        "#         water = action[2]\n",
        "\n",
        "#         # 2. Apply State\n",
        "#         ##  T_soil\n",
        "#         self.state[0] =  T_air - self.alpha * (T_air - self.state[0])\n",
        "#         ##  phi_soil\n",
        "#         self.state[1] =  phi_air + self.beta * (phi_air - self.state[1])\n",
        "#         self.state[1] = self.state[1] + water/self.V if self.state[1] + water/self.V < 1 else 1\n",
        "#         ## NPK\n",
        "#         self.state[4] += N_act + random.uniform(-1.0,0)\n",
        "#         self.state[5] += P_act + random.uniform(-1.0,0)\n",
        "#         self.state[6] += K_act + random.uniform(-1.0,0)\n",
        "#         ## pH_soil, EC_TDS\n",
        "#         self.state[2] += pH_soil + random.uniform(-1.0,0)\n",
        "#         self.state[3] += EC_TDS + random.uniform(-1.0,0)\n",
        "\n",
        "#         # 3. Get reward\n",
        "#         # Вычисление вознаграждения\n",
        "#         reward = self._get_reward(self.state)\n",
        "\n",
        "#         # Определение, является ли эпизод завершенным\n",
        "#         done = False\n",
        "#         # Set placeholder for info\n",
        "#         info = {}\n",
        "\n",
        "#         # Return step information\n",
        "#         return self.state, reward, done, info\n",
        "#   def render(self):\n",
        "#     # Визуализация текущего состояния среды\n",
        "#     pass\n",
        "\n",
        "#   def _get_reward(self, state:np.array):\n",
        "#         optimal_means = np.array([np.mean(self.optimal_space[key]) for key in self.optimal_space.keys()])\n",
        "#         # current_means = np.array([state[key] for key in state.keys()])\n",
        "#         return 1 - np.mean((optimal_means - state)**2)\n",
        "#       # if all([(self.optimal_space[key][0] <=state[key] <= self.optimal_space[key][1]).tolist()[0] for key in state.keys()]):\n",
        "#       #   return 1\n",
        "#       # else:\n",
        "#       #   optimal_means = np.array([np.mean(self.optimal_space[key]) for key in state.keys()])\n",
        "#       #   current_means = np.array([np.mean(state[key]) for key in state.keys()])\n",
        "#       #   return 1 - np.mean((optimal_means - current_means)**2)\n",
        "#   def reset(self):\n",
        "#     self.state =self.observation_space.sample()\n",
        "#     return self.state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHdjeOWuyY0t"
      },
      "outputs": [],
      "source": [
        "optimal_space_1 = {\n",
        "                # Soil\n",
        "                'T_soil': np.array([13,16]),\n",
        "                'phi_soil': np.array([0.60,0.70]),\n",
        "                'pH_soil': np.array([5,6.5]),\n",
        "                'N': np.array([120*1e-5,150*1e-5]),\n",
        "                'P': np.array([60*1e-5,90*1e-5]),\n",
        "                'K': np.array([120*1e-5,150*1e-5]),\n",
        "                'EC_TDS': np.array([1.2,1.5]),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_teiDCnBETnr",
        "outputId": "f0626f13-c301-41f8-b764-dad598b8e0ee"
      },
      "outputs": [],
      "source": [
        "env = SoilSystemEnv(optimal_space=optimal_space_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6kAvt4fG0id"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9EHDj4DUoU8"
      },
      "source": [
        "### PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVNJjBpFj7BP"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations[0], 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions[0])\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju30O0sgITPB"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igDFJ7ljhhvg"
      },
      "outputs": [],
      "source": [
        "states = env.observation_space.shape#env.observation_space.shape\n",
        "actions = env.action_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9kQ6xWAG1hU"
      },
      "outputs": [],
      "source": [
        "model_dqn = DQN(states,actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW_pKGNcShe7"
      },
      "outputs": [],
      "source": [
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0B78X9BSTtG"
      },
      "outputs": [],
      "source": [
        "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
        "# GAMMA is the discount factor as mentioned in the previous section\n",
        "# EPS_START is the starting value of epsilon\n",
        "# EPS_END is the final value of epsilon\n",
        "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "# TAU is the update rate of the target network\n",
        "# LR is the learning rate of the ``AdamW`` optimizer\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.shape\n",
        "# Get the number of state observations\n",
        "state, info = env.reset()\n",
        "n_observations = env.observation_space.shape\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oicjqJtnERcd",
        "outputId": "95c6ff35-0842-48a1-e0ff-a056258421d1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Размерность состояния\n",
        "state_dim = 3\n",
        "# Размерность действий\n",
        "action_dim = 2\n",
        "\n",
        "# Генерация случайного вектора состояния\n",
        "state = np.random.rand(state_dim)\n",
        "print(\"Состояние:\", state)\n",
        "\n",
        "# Генерация случайного вектора действий\n",
        "actions = np.random.rand(action_dim)\n",
        "print(\"Действия:\", actions)\n",
        "\n",
        "# Вычисление оценок действий для каждой компоненты состояния\n",
        "q_values = np.zeros((state_dim, action_dim))\n",
        "for i in range(state_dim):\n",
        "    for j in range(action_dim):\n",
        "        # Простой пример: оценка Q-value - сумма компонент состояния и действия\n",
        "        q_values[i, j] = np.sum(state[i] * actions[j])\n",
        "\n",
        "print(\"Оценки Q-value:\")\n",
        "print(q_values)\n",
        "\n",
        "# Выбор наилучших действий для каждой компоненты состояния\n",
        "best_actions = np.argmax(q_values, axis=0)\n",
        "print(\"Наилучшие действия для каждой компоненты состояния:\", best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsfuOgleS3ko"
      },
      "outputs": [],
      "source": [
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        # Случайно выбираем действие с вероятностью epsilon\n",
        "        action = np.random.rand(num_actions)\n",
        "    else:\n",
        "        # Иначе выбираем действие с наибольшей оценкой Q\n",
        "        # Это предполагает, что у вас есть оценки Q для каждой компоненты действия\n",
        "        q_values = calculate_q_values(state)  # Замените на вашу реализацию оценок Q\n",
        "        action = np.argmax(q_values)\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voXVibPATEye"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltYaMAWdTGgX"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "lbtBnu9cTXlZ",
        "outputId": "54a186db-424b-4528-d099-525b32b31008"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    num_episodes = 600\n",
        "else:\n",
        "    num_episodes = 50\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and get it's state\n",
        "    state, info = env.reset()\n",
        "    print(state,info)\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    print(state,info)\n",
        "    for t in count():\n",
        "        action = select_action(state)\n",
        "        print(action)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action)\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHQqNAkVUwAg"
      },
      "source": [
        "### [From git](https://github.com/nicknochnack/OpenAI-Reinforcement-Learning-with-Custom-Environment/blob/main/OpenAI%20Custom%20Environment%20Reinforcement%20Learning.ipynb) (TensorFlow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BInYmgY7WJeN"
      },
      "source": [
        "Build Agent with Keras-RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwRuNRdShEWR"
      },
      "outputs": [],
      "source": [
        "def build_agent(\n",
        "    env,\n",
        "    gamma:float=.89,\n",
        "    batch_size:int=64,\n",
        "    target_model_update:float=1e-3,\n",
        "    MemoryLimit:int=100000,\n",
        "    theta:float=0.15\n",
        "    ):\n",
        "    actor = Sequential()\n",
        "    actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "    actor.add(Dense(24, activation='relu'))\n",
        "    actor.add(Dense(16, activation='relu'))\n",
        "    actor.add(Dense(env.action_space.shape[0], activation='sigmoid'))\n",
        "\n",
        "    # Определение архитектуры критика\n",
        "    action_input = Input(shape=(env.action_space.shape[0],))\n",
        "    observation_input = Input(shape=(1,) + env.observation_space.shape)\n",
        "    flattened_observation = Flatten()(observation_input)\n",
        "\n",
        "    x = Concatenate()([action_input, flattened_observation])\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(1, activation='linear')(x)\n",
        "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
        "\n",
        "    # Определение параметров и создание агента\n",
        "    memory = SequentialMemory(limit=MemoryLimit, window_length=1)\n",
        "    random_process = OrnsteinUhlenbeckProcess(size=env.action_space.shape[0],theta=theta)\n",
        "    agent = DDPGAgent(    actor=actor,\n",
        "        critic=critic,    critic_action_input=action_input,\n",
        "        memory=memory,    nb_actions=env.action_space.shape[0],\n",
        "        random_process=random_process,    gamma=gamma,\n",
        "        batch_size=batch_size,    target_model_update=target_model_update\n",
        "    )\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = build_agent(env)\n",
        "# Компиляция агента\n",
        "agent.compile(Adam(learning_rate=1e-6, clipnorm=1.0), metrics=['mae'])\n",
        "# Обучение агента (здесь вы должны использовать свои данные обучения)\n",
        "agent.fit(env, nb_steps=100000, visualize=False, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import product  \n",
        "# GridSearch \n",
        "param_grid = { \n",
        "    'gamma': np.linspace(0.75, 0.99).tolist(), \n",
        "    'target_model_update': np.linspace(1e-2 - (1e-2)/4, 1e-3 + (1e-2)/4).tolist(), \n",
        "    'MemoryLimit': list(map(int, range(100000, 120000, 10000))), \n",
        "    'theta': np.linspace(0.05,0.15).tolist()\n",
        "} \n",
        " \n",
        "# Создайте список всех комбинаций параметров \n",
        "param_combinations = list(product(*param_grid.values())) \n",
        "param_names = list(param_grid.keys()) \n",
        " \n",
        "# Инициализируйте переменные для хранения наилучших результатов \n",
        "best_reward = float('-inf') \n",
        "best_params = None \n",
        "best_model = None \n",
        " \n",
        "test_state = env.observation_space.sample() \n",
        "n_steps = 20 \n",
        " \n",
        "# Итерируйтесь по всем комбинациям параметров \n",
        "for params in param_combinations: \n",
        "    try:\n",
        "        agent_params = dict(zip(param_names, params)) \n",
        "        print(f'params: {agent_params}')\n",
        "        agent = build_agent(env, **agent_params) \n",
        "        agent.compile(Adam(learning_rate=1e-6, clipnorm=1.0), metrics=['mae']) \n",
        "        agent.memory = SequentialMemory(limit=agent_params['MemoryLimit'], window_length=1) \n",
        "        \n",
        "        # Обучение агента (здесь вы должны использовать свои данные обучения) \n",
        "        agent.fit(env, nb_steps=100000, visualize=False, verbose=1)\n",
        "\n",
        "        # Test\n",
        "        EPS = 1e-2\n",
        "        state = test_state\n",
        "        step=1\n",
        "        \n",
        "        action = agent.forward(state)\n",
        "        new_observation, reward, done, info = env.step(action)\n",
        "        \n",
        "        while abs(reward - 1) > EPS or step < n_steps:\n",
        "            action = agent.forward(state)\n",
        "            new_observation, reward, done, info = env.step(action)\n",
        "            state = new_observation\n",
        "            step += 1\n",
        "            \n",
        "        print(f'reward = {reward}')\n",
        "        \n",
        "        \n",
        "        # Проверьте, является ли текущая модель лучшей \n",
        "        if reward > best_reward: \n",
        "            best_reward = reward \n",
        "            best_params = agent_params \n",
        "            best_model = agent \n",
        "    except Exception as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Выведите наилучшие параметры и результаты \n",
        "print(\"Best Parameters:\", best_params) \n",
        "print(\"Best reward:\", best_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Пример выполнения действия с агентом# \n",
        "action = agent.forward(env.observation_space.sample())\n",
        "new_observation, reward, done, info = env.step(action)\n",
        "# Сохранение модели агента# \n",
        "agent.save_weights('ddpg_SOIL_weights.h5f')\n",
        "# Загрузка модели агента\n",
        "agent.load_weights('ddpg_SOIL_weights.h5f')\n",
        "# Теперь агент готов для "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_state=env.observation_space.sample()\n",
        "n_steps = 20\n",
        "state = start_state\n",
        "print(\n",
        "        f'\\nSTATE_OPTIMAL:'+\n",
        "                f'\\n T_soil: {env.optimal_space[\"T_soil\"]}'+\n",
        "                f'\\n phi_soil: {env.optimal_space[\"phi_soil\"]}'+\n",
        "                f'\\n pH_soil: {env.optimal_space[\"pH_soil\"]}'+\n",
        "                # f'\\n EC_TDS: {env.optimal_space[\"EC_TDS\"]}'+\n",
        "                f'\\n [N P K]: [{env.optimal_space[\"N\"]} {env.optimal_space[\"P\"]} {env.optimal_space[\"K\"]}]'+\n",
        "                f'\\n state:'+\n",
        "                        f'\\n T_soil: {state[0]}'+#{self.state[[\"T_soil\"]]}'+\n",
        "                        f'\\n phi_soil: {state[1]}'+#{self.state[\"phi_soil\"]}'+\n",
        "                        f'\\n pH_soil: {state[2]}'+#{self.state[\"pH_soil\"]}'+\n",
        "                        # f'\\n EC_TDS: {new_observation[3]}'+#{self.state[\"EC_TDS\"]}'+\n",
        "                        f'\\n [N P K]: [{state[3]} {state[4]} {state[5]}]\\n'+#{self.state[\"N\"]} {self.state[\"P\"]} {self.state[\"K\"]}]' \n",
        "                '\\n----------------------------------------------'\n",
        "                ) \n",
        "EPS = 1e-2\n",
        "if abs(env._get_reward(state) - 1) > EPS:\n",
        "        for step in range(n_steps):\n",
        "                print(f'reward = {env._get_reward(state)}')\n",
        "                action = agent.forward(state)\n",
        "                new_observation, reward, done, info = env.step(action)\n",
        "                print(f'step {step}\\n'+\n",
        "                f' start_state:'+\n",
        "                        f'\\n T_soil: {state[0]}'+#{self.state[[\"T_soil\"]]}'+\n",
        "                        f'\\n phi_soil: {state[1]}'+#{self.state[\"phi_soil\"]}'+\n",
        "                        f'\\n pH_soil: {state[2]}'+#{self.state[\"pH_soil\"]}'+\n",
        "                        # f'\\n EC_TDS: {state[3]}'+#{self.state[\"EC_TDS\"]}'+\n",
        "                        f'\\n [N P K]: [{state[3]} {state[4]} {state[5]}]\\n'#{self.state[\"N\"]} {self.state[\"P\"]} {self.state[\"K\"]}]'+\n",
        "                        \n",
        "                f'\\n------------------------------------\\n action:\\n'+\n",
        "                f'\\n new_observation:'+\n",
        "                        f'\\n T_soil: {new_observation[0]}'+#{self.state[[\"T_soil\"]]}'+\n",
        "                        f'\\n phi_soil: {new_observation[1]}'+#{self.state[\"phi_soil\"]}'+\n",
        "                        f'\\n pH_soil: {new_observation[2]}'+#{self.state[\"pH_soil\"]}'+\n",
        "                        # f'\\n EC_TDS: {new_observation[3]}'+#{self.state[\"EC_TDS\"]}'+\n",
        "                        f'\\n [N P K]: [{new_observation[3]} {new_observation[4]} {new_observation[5]}]\\n'+#{self.state[\"N\"]} {self.state[\"P\"]} {self.state[\"K\"]}]'+\n",
        "                f'\\n reward={reward}'+\n",
        "                f'\\n done={done}'+\n",
        "                f'\\n info={info}'+\n",
        "                '\\n----------------------------------------------'\n",
        "                )\n",
        "                state = new_observation\n",
        "                if abs(env._get_reward(state) - 1) > EPS:\n",
        "                        break        \n",
        "else:\n",
        "        print(f'reward = {env._get_reward(state)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhfVPcwgG89n"
      },
      "source": [
        "## PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1XItRxKEoxQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"stable-baselines3\"\n",
        "!pip install 'shimmy>=0.2.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbXDNXBSKV67"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "\n",
        "# Neural network for predicting action values\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "\n",
        "    def __init__(self, env, features_dim: int=128):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        # CxHxW images (channels first)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()\n",
        "            ).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "YgMgXQNPEmQu",
        "outputId": "308c8ccf-6e1c-44b0-8acc-6ec56ce09f2e"
      },
      "outputs": [],
      "source": [
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomCNN,\n",
        ")\n",
        "\n",
        "# Initialize agent\n",
        "model = PPO(\"CnnPolicy\", env.observation_space, policy_kwargs=policy_kwargs, verbose=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
