код среды
```
class SoilSystemEnv(gym.Env):
  def __init__(
      self,
      optimal_space:dict,#{"Space_Var":[opt_start=-np.inf,opt_end=np.inf]},
      V:float=1, # объем горшка,
      alpha:float=0.2, # для расчета температуры почвы: T_soil` = T_air - alpha(T_air - T_soil) alpha in [0.1 0.3]
      beta:float=0.3, # для расчета влажности почвы: зрш` = phi_soil` + beta(phi_air - phi_soil)*100 alpha in [0.1 0.5]
      random_state:int=42,
      eps:float=1e-5,
      k:float=1e-3,
      n_steps=500
      ):
        # Actions we can take, down, stay, up
        #  В качестве действия агент(ы) могут подкручивать три винтиля (состава растворов №1 и №2, количество добавляемой воды до 1 литра) +
        self.action_space = spaces.Box(
          low=np.array([  -1,  -1,  -1, -1,  -1]),
          high=np.array([ 1,  1,  1,  1,  1]), # Valve_NPK; Valve_pH|_EC_TDS; Valve_Water; T_air; phi_air
          shape=(5,),dtype=float) 
        self.observation_space = spaces.Box(
          low=
          np.array([0,  0,  0,    40*1e-5, 40*1e-5, 40*1e-5]), # T_soil phi_soil pH_soil EC_TDS N P K
          high=
          np.array([35,  1,  1,  200*1e-5,  200*1e-5,  200*1e-5]),
          shape=(6,),dtype=float)

        self.optimal_space = {
                # Soil
                'T_soil': np.array([optimal_space['T_soil'][0],optimal_space['T_soil'][1]]),
                'phi_soil': np.array([optimal_space['phi_soil'][0],optimal_space['phi_soil'][1]]),
                'pH_soil': np.array([optimal_space['pH_soil'][0],optimal_space['pH_soil'][1]]),
                # 'EC_TDS': np.array([optimal_space['EC_TDS'][0],optimal_space['EC_TDS'][1]]),
                'N': np.array([optimal_space['N'][0],optimal_space['N'][1]]),
                'P': np.array([optimal_space['P'][0],optimal_space['P'][1]]),
                'K': np.array([optimal_space['K'][0],optimal_space['K'][1]]),
        }
        self.state = self.observation_space.sample() 
        self.V = V
        self.k=k
        self.MAX_steps = n_steps
        self.steps = 0

        self.rewards = []
        self.done = []
        # Valve 1 params (random)
        self.valve_1_N = np.array([np.mean(self.optimal_space[param]) - np.min(np.concatenate([self.optimal_space[par] for  par in ['N','P','K']])) for param in ['N','P','K']]) / \
          (np.max(np.concatenate([self.optimal_space[par] for  par in ['N','P','K']])) - np.min(np.concatenate([self.optimal_space[par] for  par in ['N','P','K']]))) # N, P, K

        # Soil propereties
        self.alpha = alpha
        self.beta = beta
        self.eps = eps
        print(
            f'valve_1_N: {self.valve_1_N}'+
            # f'\nvalve_2_N: {self.valve_2_N}'+
            f'\nalpha: {self.alpha}'+
            f'\nbeta: {self.beta}'+
            
            f'\nSTATE_START:'+
            f'\n T_soil: {self.state[0]}'+#{self.state[["T_soil"]]}'+
            f'\n phi_soil: {self.state[1]}'+#{self.state["phi_soil"]}'+
            f'\n pH_soil: {self.state[2]}'+#{self.state["pH_soil"]}'+
            # f'\n EC_TDS: {self.state[3]}'+#{self.state["EC_TDS"]}'+
            f'\n [N P K]: [{self.state[3]} {self.state[4]} {self.state[5]}]'#{self.state["N"]} {self.state["P"]} {self.state["K"]}]'+
            
            f'\nSTATE_OPTIMAL:'+
            f'\n T_soil: {self.optimal_space["T_soil"]}'+
            f'\n phi_soil: {self.optimal_space["phi_soil"]}'+
            f'\n pH_soil: {self.optimal_space["pH_soil"]}'+
            # f'\n EC_TDS: {self.optimal_space["EC_TDS"]}'+
            f'\n [N P K]: [{self.optimal_space["N"]} {self.optimal_space["P"]} {self.optimal_space["K"]}]'
              )

  def run(self,action):
    if self.steps < self.MAX_steps:
      # 1. Apply action
      #№  Get new params
      T_air = 18 + action[-2] * 12
      water = action[2] * self.V

      phi_air = action[-1]

      N_act, P_act, K_act = action[0] * self.valve_1_N * water
      pH_soil = action[1]#, EC_TDS = action[1] * self.valve_2_N*water
      # pH_soil = action[1]  

      # 2. Apply State
      ##  T_soil
      self.state[0] =  T_air - self.alpha * (T_air - self.state[0])
      ##  phi_soil
      self.state[1] =  phi_air + self.beta * (phi_air - self.state[1])
      self.state[1] = self.state[1] + water/self.V if self.state[1] + water/self.V < 1 else 1
      ## NPK
      self.state[3] += N_act + random.uniform(-.10,0)
      self.state[4] += P_act + random.uniform(-.10,0)
      self.state[5] += K_act + random.uniform(-.10,0)
      ## pH_soil, EC_TDS
      self.state[2] += pH_soil + random.uniform(-.10,0)
      # self.state[3] += EC_TDS + random.uniform(-1.0,0)

      # 3. Get reward
      # Вычисление вознаграждения
      reward = self._get_reward(self.state)
      self.rewards.append(reward)
      return False
    else:
      return True

  def step(self,action):
        self.done.append(self.run(action))
        self.steps += 1
        info = {}
        return self.state, self.rewards[-1], self.done[-1], info
  def render(self):
    # Визуализация текущего состояния среды
    pass
  def _get_reward(self, state):     
      keys = ['T_soil','phi_soil','pH_soil','N','P','K']
      optimal_means = np.array([np.mean(self.optimal_space[key]) for key in keys])
      err = optimal_means - state
      return np.clip(1 - np.mean(err)**2,-1,1)

  def reset(self):
    self.state =self.observation_space.sample()
    self.steps = 0
    self.rewards = []
    return self.state

```
код создания и обучения среды
```
optimal_space_1 = {
                # Soil
                'T_soil': np.array([13,16]),
                'phi_soil': np.array([0.60,0.70]),
                'pH_soil': np.array([5,6.5]),
                'N': np.array([120*1e-5,150*1e-5]),
                'P': np.array([60*1e-5,90*1e-5]),
                'K': np.array([120*1e-5,150*1e-5]),
                'EC_TDS': np.array([1.2,1.5]),
        }
env = SoilSystemEnv(optimal_space=optimal_space_1)

def build_agent(
    env,
    gamma:float=.89,
    batch_size:int=64,
    target_model_update:float=1e-3,
    MemoryLimit:int=100000,
    theta:float=0.15
    ):
    actor = Sequential()
    actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))
    actor.add(Dense(24, activation='relu'))
    actor.add(Dense(16, activation='relu'))
    actor.add(Dense(env.action_space.shape[0], activation='sigmoid'))

    # Определение архитектуры критика
    action_input = Input(shape=(env.action_space.shape[0],))
    observation_input = Input(shape=(1,) + env.observation_space.shape)
    flattened_observation = Flatten()(observation_input)

    x = Concatenate()([action_input, flattened_observation])
    x = Dense(32, activation='relu')(x)
    x = Dense(32, activation='relu')(x)
    x = Dense(1, activation='linear')(x)
    critic = Model(inputs=[action_input, observation_input], outputs=x)

    # Определение параметров и создание агента
    memory = SequentialMemory(limit=MemoryLimit, window_length=1)
    random_process = OrnsteinUhlenbeckProcess(size=env.action_space.shape[0],theta=theta)
    agent = DDPGAgent(    actor=actor,
        critic=critic,    critic_action_input=action_input,
        memory=memory,    nb_actions=env.action_space.shape[0],
        random_process=random_process,    gamma=gamma,
        batch_size=batch_size,    target_model_update=target_model_update
    )
    return agent

agent = build_agent(env)
# Компиляция агента
agent.compile(Adam(learning_rate=1e-6, clipnorm=1.0), metrics=['mae'])
# Обучение агента (здесь вы должны использовать свои данные обучения)
agent.fit(env, nb_steps=100000, visualize=False, verbose=1)

```
Лог обучения
```
Training for 100000 steps ...
Interval 1 (0 steps performed)
/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
10000/10000 [==============================] - 106s 10ms/step - reward: -0.9318
19 episodes - episode_reward: -465.766 [-501.000, -343.487] - loss: 0.963 - mae: 0.306 - mean_q: -7.778

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 112s 11ms/step - reward: -0.9591
20 episodes - episode_reward: -480.814 [-501.000, -330.245] - loss: 0.518 - mae: 0.309 - mean_q: -9.662

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 114s 11ms/step - reward: -0.9650
20 episodes - episode_reward: -484.964 [-501.000, -370.629] - loss: 0.214 - mae: 0.293 - mean_q: -8.592

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 117s 12ms/step - reward: -0.9407
20 episodes - episode_reward: -470.017 [-501.000, -375.180] - loss: 0.160 - mae: 0.283 - mean_q: -7.965

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 119s 12ms/step - reward: -0.9440
20 episodes - episode_reward: -475.638 [-501.000, -405.740] - loss: 0.139 - mae: 0.272 - mean_q: -7.727

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 122s 12ms/step - reward: -0.9255
20 episodes - episode_reward: -460.110 [-500.528, -361.394] - loss: 0.130 - mae: 0.264 - mean_q: -7.786

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 123s 12ms/step - reward: -0.9367
20 episodes - episode_reward: -469.419 [-501.000, -346.386] - loss: 0.134 - mae: 0.264 - mean_q: -7.896

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 128s 13ms/step - reward: -0.9413
20 episodes - episode_reward: -471.547 [-501.000, -373.944] - loss: 0.135 - mae: 0.263 - mean_q: -7.941

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 130s 13ms/step - reward: -0.9543
20 episodes - episode_reward: -478.876 [-501.000, -386.349] - loss: 0.127 - mae: 0.256 - mean_q: -7.986

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 132s 13ms/step - reward: -0.9605
done, took 1204.073 seconds
<keras.src.callbacks.History at 0x79bd43f95db0>
```
Почему награды отрицательные, почему не обучается? предложи пути решения