{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_torch import AgentDDPG\n",
    "from sac_torch import AgentSAC\n",
    "# \n",
    "import torch\n",
    "import gym\n",
    "# from SAC import SAC_Agent\n",
    "from ReplayBuffer import RandomBuffer, device\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os, shutil\n",
    "import argparse\n",
    "from Adapter import *\n",
    "#\n",
    "import gym \n",
    "import numpy as np\n",
    "from utils import plotLearning \n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from utils import show_video, convert_gif\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "    \n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_env = 'LunarLanderContinuous-v2'\n",
    "name_env =\"BipedalWalker-v3\"\n",
    "env = gym.make(name_env)\n",
    "\n",
    "print([*env.observation_space.shape],*env.action_space.shape)\n",
    "\n",
    "input_dims = [*env.observation_space.shape]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "print(input_dims,n_actions)\n",
    "\n",
    "\n",
    "def make_gif(frame_folder,name):\n",
    "      frames = [Image.open(image) for image in glob.glob(f\"{frame_folder}/*.png\")]\n",
    "      frame_one = frames[0]\n",
    "      frame_one.save(os.path.join(frame_folder,f\"{name}.gif\"), format=\"GIF\", append_images=frames,\n",
    "                save_all=True, duration=100, loop=0)    \n",
    "def render_mp4(videopath: str) -> str:\n",
    "    \"\"\"\n",
    "    Gets a string containing a b4-encoded version of the MP4 video\n",
    "    at the specified path.\n",
    "    \"\"\"\n",
    "    mp4 = open(videopath, 'rb').read()\n",
    "    base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "    return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "          f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentDDPG = AgentDDPG(\n",
    "    alpha=0.0001,\n",
    "    beta=0.00001,\n",
    "    input_dims=input_dims,\n",
    "    tau=0.001,env=env,\n",
    "    rollout_len = 500,\n",
    "    total_rollouts = 1000,\n",
    "    batch_size=64,\n",
    "    layer1_size=256,\n",
    "    layer2_size=128,\n",
    "    n_actions=n_actions,\n",
    "    agent_dir=os.path.join(os.getcwd(),name_env))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentDDPG.train(True,plot_save=os.path.join(os.getcwd(),'plots','DDPG',name_env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(os.path.join(os.getcwd(),'plots','DDPG',name_env),name_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install tensorflow\n",
    "!pip install tensorboar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "        \"state_dim\": input_dims[0],\n",
    "        \"action_dim\": n_actions,\n",
    "        \"gamma\": .99,\n",
    "        \"hid_shape\": (256,128),\n",
    "        \"a_lr\": 3e-5,\n",
    "        \"c_lr\": 3e-5,\n",
    "        \"batch_size\":128,\n",
    "        \"alpha\":0.12,\n",
    "        \"adaptive_alpha\":True\n",
    "    }\n",
    "\n",
    "agentSAC = AgentSAC(**kwargs, agent_dir=os.path.join(os.getcwd(),name_env,'SAC'))\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(),name_env,'SAC')): \n",
    "    os.mkdir(os.path.join(os.getcwd(),name_env,'SAC'))\n",
    "BriefEnvName = ['BWv3', 'BWHv3', 'Lch_Cv2', 'PV0', 'Humanv2', 'HCv2']\n",
    "\n",
    "EnvIdex = 0\n",
    "    \n",
    "random_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timenow = str(datetime.now())[0:-10]\n",
    "timenow = ' ' + timenow[0:13] + '_' + timenow[-2::]\n",
    "writepath = os.path.join(os.getcwd(),name_env,'SAC','runs',f'SAC_{BriefEnvName[EnvIdex]}' + timenow)\n",
    "if os.path.exists(writepath): \n",
    "    shutil.rmtree(writepath)\n",
    "writer= SummaryWriter(log_dir=writepath)\n",
    "print(f'writepath: {writepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "agentSAC.train(\n",
    "    env=env,total_steps=int(5e6),EnvIdex = 0,write=True,\n",
    "    writer= writer,\n",
    "    plot=True,\n",
    "    plot_path=os.path.join(os.getcwd(),name_env,'SAC','plots'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(os.path.join(os.getcwd(),name_env,'SAC','plots'),name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "ea = event_accumulator.EventAccumulator(writepath,\n",
    "       size_guidance={ # see below regarding this argument\n",
    "       event_accumulator.COMPRESSED_HISTOGRAMS: 500,\n",
    "       event_accumulator.IMAGES: 4,\n",
    "       event_accumulator.AUDIO: 4,\n",
    "       event_accumulator.SCALARS: 0,\n",
    "       event_accumulator.HISTOGRAMS: 1,\n",
    "       })\n",
    "ea.Reload() # loads events from file\n",
    "# Out[3]: <tensorflow.python.summary.event_accumulator.EventAccumulator at 0x7fdbe5ff59e8>\n",
    "ea.Tags() \n",
    "ea.Scalars('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "from selenium import webdriver\n",
    "\n",
    "display = Display(visible=0, size=(800, 600))\n",
    "display.start()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--no-sandbox')  # May be required in some environments\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')  # May be required in some environments\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_dir = os.path.join(f\"{name_env}\",f\"{name_env}_before_training.mp4\")\n",
    "# # if not os.path.exists(videos_dir):\n",
    "# #     # Create a new directory because it does not exist\n",
    "# #     os.makedirs(videos_dir)\n",
    "# #     print(\"The new directory is created!\")\n",
    "    \n",
    "# before_training = videos_dir\n",
    "\n",
    "# video = VideoRecorder(env, before_training)\n",
    "# # returns an initial observation\n",
    "# env.reset()\n",
    "# for i in range(200):\n",
    "#   env.render()\n",
    "#   video.capture_frame()\n",
    "#   # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "#   new_state,reward,done,info,_ =  env.step(env.action_space.sample())\n",
    "#   # Not printing this time\n",
    "#   #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "# video.close()\n",
    "# env.close()\n",
    "\n",
    "before_training = \"before_training.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  act = env.action_space.sample() \n",
    "  observation, reward, done, info,_ = env.step(act)\n",
    "  # Not printing this time\n",
    "  #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "html = render_mp4(before_training)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(plot_save):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(plot_save)\n",
    "    print(\"The new directory is created!\")\n",
    "for i in range(1000):\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = env.reset()[0]\n",
    "    # print(obs)\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs) \n",
    "        new_state,reward,done,info,_ = env.step(act)\n",
    "        agent.remember(state=obs,action=act,reward=reward,new_state=new_state,done=done)\n",
    "        \n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "    score_history.append(score)\n",
    "    print(f'Episode {i} score {round(score, 2)} 100 game average {round(np.mean(score_history[-100:]), 2)}')\n",
    "    if i % 25 == 0:\n",
    "        agent.save_models()   \n",
    "    filename = f'{i}.png'\n",
    "    plotLearning(score_history,os.path.join(plot_save,filename),window=100)\n",
    "make_gif(plot_save,name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "before_training = \"before_training.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  observation, reward, done, info = env.step(env.action_space.sample())\n",
    "  # Not printing this time\n",
    "  #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_name in [\n",
    "    'LunarLanderContinuous-v2',\n",
    "    \"BipedalWalker-v3\"]:\n",
    "    env = gym.make(env_name)\n",
    "    input_dims = [*env.observation_space.shape]\n",
    "    n_actions = env.action_space.shape[0]   \n",
    "    \n",
    "    agent = AgentDDPG(\n",
    "    alpha=0.000025,\n",
    "    beta=0.00025,\n",
    "    input_dims=input_dims,\n",
    "    tau=0.001,env=env,\n",
    "    batch_size=64,\n",
    "    layer1_size=400,\n",
    "    layer2_size=300,\n",
    "    n_actions=n_actions,\n",
    "    agent_dir=os.path.join(os.getcwd(),env_name))\n",
    "    \n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    score_history = []\n",
    "    plot_save = os.path.join(os.getcwd(),'plots',env_name)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(plot_save):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(plot_save)\n",
    "        print(\"The new directory is created!\")\n",
    "    for i in range(1000):\n",
    "        done = False\n",
    "        score = 0\n",
    "        obs = env.reset()[0]\n",
    "        # print(obs)\n",
    "        while not done:\n",
    "            act = agent.choose_action(obs) \n",
    "            new_state,reward,done,info,_ = env.step(act)\n",
    "            agent.remember(state=obs,action=act,reward=reward,new_state=new_state,done=done)\n",
    "            agent.learn()\n",
    "            score += reward\n",
    "            obs = new_state\n",
    "        score_history.append(score)\n",
    "        print(f'Episode {i} score {round(score, 2)} 100 game average {round(np.mean(score_history[-100:]), 2)}')\n",
    "        if i % 25 == 0:\n",
    "            agent.save_models()   \n",
    "        filename = f'{i}.png'\n",
    "        plotLearning(score_history,os.path.join(plot_save,filename),window=100)\n",
    "    make_gif(plot_save,env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
