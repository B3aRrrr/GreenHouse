{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch\n",
    "!pip install -r requirements.txt\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_torch import AgentDDPG\n",
    "import gym \n",
    "import numpy as np\n",
    "from utils import plotLearning\n",
    "import os\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from utils import show_video, convert_gif\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "    \n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_env = 'LunarLanderContinuous-v2'\n",
    "# name_env =\"BipedalWalker-v3\"\n",
    "env = gym.make(name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] 2\n",
      "[8] 2\n"
     ]
    }
   ],
   "source": [
    "print([*env.observation_space.shape],*env.action_space.shape)\n",
    "\n",
    "input_dims = [*env.observation_space.shape]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "print(input_dims,n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(frame_folder,name):\n",
    "    frames = [Image.open(image) for image in glob.glob(f\"{frame_folder}/*.png\")]\n",
    "    frame_one = frames[0]\n",
    "    frame_one.save(os.path.join(frame_folder,f\"{name}.gif\"), format=\"GIF\", append_images=frames,\n",
    "               save_all=True, duration=100, loop=0)    \n",
    "def render_mp4(videopath: str) -> str:\n",
    "  \"\"\"\n",
    "  Gets a string containing a b4-encoded version of the MP4 video\n",
    "  at the specified path.\n",
    "  \"\"\"\n",
    "  mp4 = open(videopath, 'rb').read()\n",
    "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentDDPG(\n",
    "    alpha=0.000925,\n",
    "    beta=0.0000915,\n",
    "    input_dims=input_dims,\n",
    "    tau=0.001,env=env,\n",
    "    rollout_len = 500,\n",
    "    total_rollouts = 1000,\n",
    "    batch_size=64,\n",
    "    layer1_size=400,\n",
    "    layer2_size=300,\n",
    "    n_actions=n_actions,\n",
    "    agent_dir=os.path.join(os.getcwd(),name_env))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode = 0; score = -530.7 after 163 steps; 100 game average = -530.7\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode = 1; score = -143.14 after 222 steps; 100 game average = -336.92\n",
      "Episode = 2; score = -28.56 after 124 steps; 100 game average = -234.13\n",
      "Episode = 3; score = -147.51 after 84 steps; 100 game average = -212.48\n",
      "Episode = 4; score = -297.23 after 89 steps; 100 game average = -229.43\n",
      "Episode = 5; score = -284.64 after 98 steps; 100 game average = -238.63\n",
      "Episode = 6; score = -287.61 after 113 steps; 100 game average = -245.63\n",
      "Episode = 7; score = -142.12 after 136 steps; 100 game average = -232.69\n",
      "Episode = 8; score = -309.49 after 144 steps; 100 game average = -241.22\n",
      "Episode = 9; score = -270.88 after 169 steps; 100 game average = -244.19\n",
      "Episode = 10; score = -75.19 after 157 steps; 100 game average = -228.82\n",
      "Episode = 11; score = -220.9 after 142 steps; 100 game average = -228.16\n",
      "Episode = 12; score = -275.8 after 74 steps; 100 game average = -231.83\n",
      "Episode = 13; score = -285.19 after 85 steps; 100 game average = -235.64\n",
      "Episode = 14; score = -4.91 after 81 steps; 100 game average = -220.26\n",
      "Episode = 15; score = -122.61 after 56 steps; 100 game average = -214.15\n",
      "Episode = 16; score = -139.71 after 98 steps; 100 game average = -209.77\n",
      "Episode = 17; score = -143.29 after 72 steps; 100 game average = -206.08\n",
      "Episode = 18; score = -201.53 after 88 steps; 100 game average = -205.84\n",
      "Episode = 19; score = -245.05 after 99 steps; 100 game average = -207.8\n",
      "Episode = 20; score = -84.38 after 100 steps; 100 game average = -201.92\n",
      "Episode = 21; score = -280.26 after 95 steps; 100 game average = -205.49\n",
      "Episode = 22; score = -294.84 after 120 steps; 100 game average = -209.37\n",
      "Episode = 23; score = -520.99 after 176 steps; 100 game average = -222.35\n",
      "Episode = 24; score = -178.96 after 172 steps; 100 game average = -220.62\n",
      "Episode = 25; score = -156.18 after 103 steps; 100 game average = -218.14\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode = 26; score = -319.75 after 172 steps; 100 game average = -221.9\n",
      "Episode = 27; score = -458.96 after 115 steps; 100 game average = -230.37\n",
      "Episode = 28; score = -256.69 after 97 steps; 100 game average = -231.28\n",
      "Episode = 29; score = -77.11 after 131 steps; 100 game average = -226.14\n",
      "Episode = 30; score = -119.84 after 66 steps; 100 game average = -222.71\n",
      "Episode = 31; score = -63.16 after 82 steps; 100 game average = -217.72\n",
      "Episode = 32; score = -218.7 after 110 steps; 100 game average = -217.75\n",
      "Episode = 33; score = -62.82 after 140 steps; 100 game average = -213.2\n",
      "Episode = 34; score = -127.25 after 188 steps; 100 game average = -210.74\n",
      "Episode = 35; score = -78.83 after 127 steps; 100 game average = -207.08\n",
      "Episode = 36; score = -197.5 after 176 steps; 100 game average = -206.82\n",
      "Episode = 37; score = -371.25 after 186 steps; 100 game average = -211.14\n",
      "Episode = 38; score = -281.24 after 121 steps; 100 game average = -212.94\n",
      "Episode = 39; score = -481.71 after 232 steps; 100 game average = -219.66\n",
      "Episode = 40; score = -330.75 after 134 steps; 100 game average = -222.37\n",
      "Episode = 41; score = -169.21 after 201 steps; 100 game average = -221.1\n",
      "Episode = 42; score = -315.36 after 163 steps; 100 game average = -223.3\n",
      "Episode = 43; score = -283.83 after 116 steps; 100 game average = -224.67\n",
      "Episode = 44; score = -613.65 after 147 steps; 100 game average = -233.32\n",
      "Episode = 45; score = -252.85 after 99 steps; 100 game average = -233.74\n",
      "Episode = 46; score = -479.02 after 159 steps; 100 game average = -238.96\n",
      "Episode = 47; score = -364.35 after 123 steps; 100 game average = -241.57\n",
      "Episode = 48; score = -138.6 after 127 steps; 100 game average = -239.47\n",
      "Episode = 49; score = -236.71 after 136 steps; 100 game average = -239.42\n",
      "Episode = 50; score = -28.15 after 110 steps; 100 game average = -235.27\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode = 51; score = -121.75 after 72 steps; 100 game average = -233.09\n",
      "Episode = 52; score = -194.55 after 74 steps; 100 game average = -232.36\n",
      "Episode = 53; score = -339.11 after 104 steps; 100 game average = -234.34\n",
      "Episode = 54; score = -426.39 after 88 steps; 100 game average = -237.83\n",
      "Episode = 55; score = -343.66 after 94 steps; 100 game average = -239.72\n",
      "Episode = 56; score = -80.17 after 128 steps; 100 game average = -236.92\n",
      "Episode = 57; score = -339.31 after 246 steps; 100 game average = -238.69\n",
      "Episode = 58; score = -506.57 after 157 steps; 100 game average = -243.23\n",
      "Episode = 59; score = 9.13 after 176 steps; 100 game average = -239.02\n",
      "Episode = 60; score = -810.31 after 87 steps; 100 game average = -248.39\n",
      "Episode = 61; score = -201.16 after 110 steps; 100 game average = -247.63\n",
      "Episode = 62; score = -246.19 after 139 steps; 100 game average = -247.6\n",
      "Episode = 63; score = -35.32 after 140 steps; 100 game average = -244.29\n",
      "Episode = 64; score = -105.95 after 167 steps; 100 game average = -242.16\n",
      "Episode = 65; score = -119.77 after 202 steps; 100 game average = -240.3\n",
      "Episode = 66; score = -82.51 after 101 steps; 100 game average = -237.95\n",
      "Episode = 67; score = -440.37 after 295 steps; 100 game average = -240.92\n",
      "Episode = 68; score = -178.02 after 73 steps; 100 game average = -240.01\n",
      "Episode = 69; score = -131.63 after 73 steps; 100 game average = -238.46\n",
      "Episode = 70; score = -137.21 after 60 steps; 100 game average = -237.04\n",
      "Episode = 71; score = -98.61 after 137 steps; 100 game average = -235.12\n",
      "Episode = 72; score = -187.01 after 140 steps; 100 game average = -234.46\n",
      "Episode = 73; score = -309.63 after 196 steps; 100 game average = -235.47\n"
     ]
    }
   ],
   "source": [
    "agent.train(True,plot_save=os.path.join(os.getcwd(),'plots',name_env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(os.path.join(os.getcwd(),'plots',name_env),name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "from selenium import webdriver\n",
    "\n",
    "display = Display(visible=0, size=(800, 600))\n",
    "display.start()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--no-sandbox')  # May be required in some environments\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')  # May be required in some environments\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_dir = os.path.join(f\"{name_env}\",f\"{name_env}_before_training.mp4\")\n",
    "# # if not os.path.exists(videos_dir):\n",
    "# #     # Create a new directory because it does not exist\n",
    "# #     os.makedirs(videos_dir)\n",
    "# #     print(\"The new directory is created!\")\n",
    "    \n",
    "# before_training = videos_dir\n",
    "\n",
    "# video = VideoRecorder(env, before_training)\n",
    "# # returns an initial observation\n",
    "# env.reset()\n",
    "# for i in range(200):\n",
    "#   env.render()\n",
    "#   video.capture_frame()\n",
    "#   # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "#   new_state,reward,done,info,_ =  env.step(env.action_space.sample())\n",
    "#   # Not printing this time\n",
    "#   #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "# video.close()\n",
    "# env.close()\n",
    "\n",
    "before_training = \"before_training.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  act = env.action_space.sample() \n",
    "  observation, reward, done, info,_ = env.step(act)\n",
    "  # Not printing this time\n",
    "  #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "html = render_mp4(before_training)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(plot_save):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(plot_save)\n",
    "    print(\"The new directory is created!\")\n",
    "for i in range(1000):\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = env.reset()[0]\n",
    "    # print(obs)\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs) \n",
    "        new_state,reward,done,info,_ = env.step(act)\n",
    "        agent.remember(state=obs,action=act,reward=reward,new_state=new_state,done=done)\n",
    "        \n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "    score_history.append(score)\n",
    "    print(f'Episode {i} score {round(score, 2)} 100 game average {round(np.mean(score_history[-100:]), 2)}')\n",
    "    if i % 25 == 0:\n",
    "        agent.save_models()   \n",
    "    filename = f'{i}.png'\n",
    "    plotLearning(score_history,os.path.join(plot_save,filename),window=100)\n",
    "make_gif(plot_save,name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "before_training = \"before_training.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  observation, reward, done, info = env.step(env.action_space.sample())\n",
    "  # Not printing this time\n",
    "  #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_name in [\n",
    "    'LunarLanderContinuous-v2',\n",
    "    \"BipedalWalker-v3\"]:\n",
    "    env = gym.make(env_name)\n",
    "    input_dims = [*env.observation_space.shape]\n",
    "    n_actions = env.action_space.shape[0]   \n",
    "    \n",
    "    agent = AgentDDPG(\n",
    "    alpha=0.000025,\n",
    "    beta=0.00025,\n",
    "    input_dims=input_dims,\n",
    "    tau=0.001,env=env,\n",
    "    batch_size=64,\n",
    "    layer1_size=400,\n",
    "    layer2_size=300,\n",
    "    n_actions=n_actions,\n",
    "    agent_dir=os.path.join(os.getcwd(),env_name))\n",
    "    \n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    score_history = []\n",
    "    plot_save = os.path.join(os.getcwd(),'plots',env_name)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(plot_save):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(plot_save)\n",
    "        print(\"The new directory is created!\")\n",
    "    for i in range(1000):\n",
    "        done = False\n",
    "        score = 0\n",
    "        obs = env.reset()[0]\n",
    "        # print(obs)\n",
    "        while not done:\n",
    "            act = agent.choose_action(obs) \n",
    "            new_state,reward,done,info,_ = env.step(act)\n",
    "            agent.remember(state=obs,action=act,reward=reward,new_state=new_state,done=done)\n",
    "            agent.learn()\n",
    "            score += reward\n",
    "            obs = new_state\n",
    "        score_history.append(score)\n",
    "        print(f'Episode {i} score {round(score, 2)} 100 game average {round(np.mean(score_history[-100:]), 2)}')\n",
    "        if i % 25 == 0:\n",
    "            agent.save_models()   \n",
    "        filename = f'{i}.png'\n",
    "        plotLearning(score_history,os.path.join(plot_save,filename),window=100)\n",
    "    make_gif(plot_save,env_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
