{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -r requirements.txt\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_torch import AgentDDPG\n",
    "from sac_torch import AgentSAC\n",
    "# \n",
    "import torch\n",
    "import gym\n",
    "# from SAC import SAC_Agent\n",
    "from ReplayBuffer import RandomBuffer, device\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os, shutil\n",
    "import argparse\n",
    "from Adapter import *\n",
    "#\n",
    "import gym \n",
    "import numpy as np\n",
    "from utils import plotLearning \n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from utils import show_video, convert_gif\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "    \n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24] 4\n",
      "[24] 4\n"
     ]
    }
   ],
   "source": [
    "# name_env = 'LunarLanderContinuous-v2'\n",
    "name_env =\"BipedalWalker-v3\"\n",
    "env = gym.make(name_env)\n",
    "\n",
    "print([*env.observation_space.shape],*env.action_space.shape)\n",
    "\n",
    "input_dims = [*env.observation_space.shape]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "print(input_dims,n_actions)\n",
    "\n",
    "\n",
    "def make_gif(frame_folder,name):\n",
    "      frames = [Image.open(image) for image in glob.glob(f\"{frame_folder}/*.png\")]\n",
    "      frame_one = frames[0]\n",
    "      frame_one.save(os.path.join(frame_folder,f\"{name}.gif\"), format=\"GIF\", append_images=frames,\n",
    "                save_all=True, duration=100, loop=0)    \n",
    "def render_mp4(videopath: str) -> str:\n",
    "    \"\"\"\n",
    "    Gets a string containing a b4-encoded version of the MP4 video\n",
    "    at the specified path.\n",
    "    \"\"\"\n",
    "    mp4 = open(videopath, 'rb').read()\n",
    "    base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "    return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "          f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentDDPG = AgentDDPG(\n",
    "    alpha=0.0001,\n",
    "    beta=0.00001,\n",
    "    input_dims=input_dims,\n",
    "    tau=0.001,env=env,\n",
    "    rollout_len = 500,\n",
    "    total_rollouts = 1000,\n",
    "    batch_size=64,\n",
    "    layer1_size=256,\n",
    "    layer2_size=128,\n",
    "    n_actions=n_actions,\n",
    "    agent_dir=os.path.join(os.getcwd(),name_env))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentDDPG.train(True,plot_save=os.path.join(os.getcwd(),'plots','DDPG',name_env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(os.path.join(os.getcwd(),'plots','DDPG',name_env),name_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install tensorflow\n",
    "!pip install tensorboar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "        \"state_dim\": input_dims[0],\n",
    "        \"action_dim\": n_actions,\n",
    "        \"gamma\": .99,\n",
    "        \"hid_shape\": (256,128),\n",
    "        \"a_lr\": 3e-5,\n",
    "        \"c_lr\": 3e-5,\n",
    "        \"batch_size\":128,\n",
    "        \"alpha\":0.12,\n",
    "        \"adaptive_alpha\":True\n",
    "    }\n",
    "\n",
    "agentSAC = AgentSAC(**kwargs, agent_dir=os.path.join(os.getcwd(),name_env,'SAC'))\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(),name_env,'SAC')): \n",
    "    os.mkdir(os.path.join(os.getcwd(),name_env,'SAC'))\n",
    "BriefEnvName = ['BWv3', 'BWHv3', 'Lch_Cv2', 'PV0', 'Humanv2', 'HCv2']\n",
    "\n",
    "EnvIdex = 0\n",
    "    \n",
    "random_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writepath: c:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\BipedalWalker-v3\\SAC\\runs\\SAC_BWv3 2023-09-20 19_04\n"
     ]
    }
   ],
   "source": [
    "timenow = str(datetime.now())[0:-10]\n",
    "timenow = ' ' + timenow[0:13] + '_' + timenow[-2::]\n",
    "writepath = os.path.join(os.getcwd(),name_env,'SAC','runs',f'SAC_{BriefEnvName[EnvIdex]}' + timenow)\n",
    "if os.path.exists(writepath): \n",
    "    shutil.rmtree(writepath)\n",
    "writer= SummaryWriter(log_dir=writepath)\n",
    "print(f'writepath: {writepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_every = 50;eval_interval = 1000;save_interval = 2500;max_action = 1.0steps_per_epoch = 1600;start_steps = 8000;update_after = 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\SoilControlModel_v2_test.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/GreenHouse/SoilControlModel_v2_test.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/GreenHouse/SoilControlModel_v2_test.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m agentSAC\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/GreenHouse/SoilControlModel_v2_test.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     env\u001b[39m=\u001b[39;49menv,total_steps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(\u001b[39m5e6\u001b[39;49m),EnvIdex \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,write\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/GreenHouse/SoilControlModel_v2_test.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     writer\u001b[39m=\u001b[39;49m writer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/GreenHouse/SoilControlModel_v2_test.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     plot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dmitry/Desktop/Lab5/GreenHouse/SoilControlModel_v2_test.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     plot_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetcwd(),name_env,\u001b[39m'\u001b[39;49m\u001b[39mSAC\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mplots\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\sac_torch.py:360\u001b[0m, in \u001b[0;36mAgentSAC.train\u001b[1;34m(self, env, total_steps, EnvIdex, write, writer, EnvName, update_every, eval_interval, save_interval, random_seed, plot, plot_path)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''record & log'''\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[39mif\u001b[39;00m (t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 360\u001b[0m     score \u001b[39m=\u001b[39m evaluate_policy(eval_env, \u001b[39mself\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m, steps_per_epoch, max_action, EnvIdex)\n\u001b[0;32m    361\u001b[0m     \u001b[39mif\u001b[39;00m write:\n\u001b[0;32m    362\u001b[0m         writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mep_r\u001b[39m\u001b[39m'\u001b[39m, score, global_step\u001b[39m=\u001b[39mt \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\sac_torch.py:53\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(env, model, render, steps_per_epoch, max_action, EnvIdex)\u001b[0m\n\u001b[0;32m     49\u001b[0m s, done, ep_r \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()[\u001b[39m0\u001b[39m], \u001b[39mFalse\u001b[39;00m, \u001b[39m0\u001b[39m\n\u001b[0;32m     50\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     51\u001b[0m     \u001b[39m# Take deterministic actions at test time\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[39m# print(f'[evaluate_policy] s= {s}')\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     a \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mselect_action(s, deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, with_logprob\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     54\u001b[0m     act \u001b[39m=\u001b[39m Action_adapter(a, max_action)  \u001b[39m# [0,1] to [-max,max]\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     s_prime, r, done, info,_ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(act)\u001b[39m#s_prime, r, done, info = env.step(act)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\sac_torch.py:244\u001b[0m, in \u001b[0;36mAgentSAC.select_action\u001b[1;34m(self, state, deterministic, with_logprob)\u001b[0m\n\u001b[0;32m    242\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(state)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    243\u001b[0m     \u001b[39m# print(f'[AgentSAC] state={state}')\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     a, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(state, deterministic, with_logprob)\n\u001b[0;32m    245\u001b[0m \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\sac_torch.py:159\u001b[0m, in \u001b[0;36mActor.forward\u001b[1;34m(self, state, deterministic, with_logprob)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state, deterministic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, with_logprob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    158\u001b[0m \u001b[39m\t\u001b[39m\u001b[39m'''Network with Enforcing Action Bounds'''\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \tnet_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ma_net(state)\n\u001b[0;32m    160\u001b[0m \tmu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmu_layer(net_out)\n\u001b[0;32m    161\u001b[0m \tlog_std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_std_layer(net_out)\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Dmitry\\Desktop\\Lab5\\GreenHouse\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "agentSAC.train(\n",
    "    env=env,total_steps=int(5e6),EnvIdex = 0,write=True,\n",
    "    writer= writer,\n",
    "    plot=True,\n",
    "    plot_path=os.path.join(os.getcwd(),name_env,'SAC','plots'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(os.path.join(os.getcwd(),name_env,'SAC','plots'),name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "ea = event_accumulator.EventAccumulator(writepath,\n",
    "       size_guidance={ # see below regarding this argument\n",
    "       event_accumulator.COMPRESSED_HISTOGRAMS: 500,\n",
    "       event_accumulator.IMAGES: 4,\n",
    "       event_accumulator.AUDIO: 4,\n",
    "       event_accumulator.SCALARS: 0,\n",
    "       event_accumulator.HISTOGRAMS: 1,\n",
    "       })\n",
    "ea.Reload() # loads events from file\n",
    "# Out[3]: <tensorflow.python.summary.event_accumulator.EventAccumulator at 0x7fdbe5ff59e8>\n",
    "ea.Tags() \n",
    "ea.Scalars('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "from selenium import webdriver\n",
    "\n",
    "display = Display(visible=0, size=(800, 600))\n",
    "display.start()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--no-sandbox')  # May be required in some environments\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')  # May be required in some environments\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_dir = os.path.join(f\"{name_env}\",f\"{name_env}_before_training.mp4\")\n",
    "# # if not os.path.exists(videos_dir):\n",
    "# #     # Create a new directory because it does not exist\n",
    "# #     os.makedirs(videos_dir)\n",
    "# #     print(\"The new directory is created!\")\n",
    "    \n",
    "# before_training = videos_dir\n",
    "\n",
    "# video = VideoRecorder(env, before_training)\n",
    "# # returns an initial observation\n",
    "# env.reset()\n",
    "# for i in range(200):\n",
    "#   env.render()\n",
    "#   video.capture_frame()\n",
    "#   # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "#   new_state,reward,done,info,_ =  env.step(env.action_space.sample())\n",
    "#   # Not printing this time\n",
    "#   #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "# video.close()\n",
    "# env.close()\n",
    "\n",
    "before_training = \"before_training.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  act = env.action_space.sample() \n",
    "  observation, reward, done, info,_ = env.step(act)\n",
    "  # Not printing this time\n",
    "  #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "html = render_mp4(before_training)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(plot_save):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(plot_save)\n",
    "    print(\"The new directory is created!\")\n",
    "for i in range(1000):\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = env.reset()[0]\n",
    "    # print(obs)\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs) \n",
    "        new_state,reward,done,info,_ = env.step(act)\n",
    "        agent.remember(state=obs,action=act,reward=reward,new_state=new_state,done=done)\n",
    "        \n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "    score_history.append(score)\n",
    "    print(f'Episode {i} score {round(score, 2)} 100 game average {round(np.mean(score_history[-100:]), 2)}')\n",
    "    if i % 25 == 0:\n",
    "        agent.save_models()   \n",
    "    filename = f'{i}.png'\n",
    "    plotLearning(score_history,os.path.join(plot_save,filename),window=100)\n",
    "make_gif(plot_save,name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "before_training = \"before_training.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  observation, reward, done, info = env.step(env.action_space.sample())\n",
    "  # Not printing this time\n",
    "  #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_name in [\n",
    "    'LunarLanderContinuous-v2',\n",
    "    \"BipedalWalker-v3\"]:\n",
    "    env = gym.make(env_name)\n",
    "    input_dims = [*env.observation_space.shape]\n",
    "    n_actions = env.action_space.shape[0]   \n",
    "    \n",
    "    agent = AgentDDPG(\n",
    "    alpha=0.000025,\n",
    "    beta=0.00025,\n",
    "    input_dims=input_dims,\n",
    "    tau=0.001,env=env,\n",
    "    batch_size=64,\n",
    "    layer1_size=400,\n",
    "    layer2_size=300,\n",
    "    n_actions=n_actions,\n",
    "    agent_dir=os.path.join(os.getcwd(),env_name))\n",
    "    \n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    score_history = []\n",
    "    plot_save = os.path.join(os.getcwd(),'plots',env_name)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(plot_save):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(plot_save)\n",
    "        print(\"The new directory is created!\")\n",
    "    for i in range(1000):\n",
    "        done = False\n",
    "        score = 0\n",
    "        obs = env.reset()[0]\n",
    "        # print(obs)\n",
    "        while not done:\n",
    "            act = agent.choose_action(obs) \n",
    "            new_state,reward,done,info,_ = env.step(act)\n",
    "            agent.remember(state=obs,action=act,reward=reward,new_state=new_state,done=done)\n",
    "            agent.learn()\n",
    "            score += reward\n",
    "            obs = new_state\n",
    "        score_history.append(score)\n",
    "        print(f'Episode {i} score {round(score, 2)} 100 game average {round(np.mean(score_history[-100:]), 2)}')\n",
    "        if i % 25 == 0:\n",
    "            agent.save_models()   \n",
    "        filename = f'{i}.png'\n",
    "        plotLearning(score_history,os.path.join(plot_save,filename),window=100)\n",
    "    make_gif(plot_save,env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
