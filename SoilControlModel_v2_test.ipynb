{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch\n",
    "!pip install -r requirements.txt\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_torch import AgentDDPG\n",
    "import gym \n",
    "import numpy as np\n",
    "from utils import plotLearning\n",
    "import os\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from utils import show_video, convert_gif\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "    \n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_env = 'LunarLanderContinuous-v2'\n",
    "# name_env =\"BipedalWalker-v3\"\n",
    "env = gym.make(name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] 2\n",
      "[8] 2\n"
     ]
    }
   ],
   "source": [
    "print([*env.observation_space.shape],*env.action_space.shape)\n",
    "\n",
    "input_dims = [*env.observation_space.shape]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "print(input_dims,n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(frame_folder,name):\n",
    "    frames = [Image.open(image) for image in glob.glob(f\"{frame_folder}/*.png\")]\n",
    "    frame_one = frames[0]\n",
    "    frame_one.save(os.path.join(frame_folder,f\"{name}.gif\"), format=\"GIF\", append_images=frames,\n",
    "               save_all=True, duration=100, loop=0)    \n",
    "def render_mp4(videopath: str) -> str:\n",
    "  \"\"\"\n",
    "  Gets a string containing a b4-encoded version of the MP4 video\n",
    "  at the specified path.\n",
    "  \"\"\"\n",
    "  mp4 = open(videopath, 'rb').read()\n",
    "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentDDPG(\n",
    "    alpha=0.000925,\n",
    "    beta=0.0000915,\n",
    "    input_dims=input_dims,\n",
    "    tau=0.001,env=env,\n",
    "    rollout_len = 500,\n",
    "    total_rollouts = 1000,\n",
    "    batch_size=64,\n",
    "    layer1_size=400,\n",
    "    layer2_size=300,\n",
    "    n_actions=n_actions,\n",
    "    agent_dir=os.path.join(os.getcwd(),name_env))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(True,plot_save=os.path.join(os.getcwd(),'plots',name_env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(os.path.join(os.getcwd(),'plots',name_env),name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "from selenium import webdriver\n",
    "\n",
    "display = Display(visible=0, size=(800, 600))\n",
    "display.start()\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--no-sandbox')  # May be required in some environments\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')  # May be required in some environments\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_dir = os.path.join(f\"{name_env}\",f\"{name_env}_before_training.mp4\")\n",
    "# # if not os.path.exists(videos_dir):\n",
    "# #     # Create a new directory because it does not exist\n",
    "# #     os.makedirs(videos_dir)\n",
    "# #     print(\"The new directory is created!\")\n",
    "    \n",
    "# before_training = videos_dir\n",
    "\n",
    "# video = VideoRecorder(env, before_training)\n",
    "# # returns an initial observation\n",
    "# env.reset()\n",
    "# for i in range(200):\n",
    "#   env.render()\n",
    "#   video.capture_frame()\n",
    "#   # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "#   new_state,reward,done,info,_ =  env.step(env.action_space.sample())\n",
    "#   # Not printing this time\n",
    "#   #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "# video.close()\n",
    "# env.close()\n",
    "\n",
    "before_training = \"before_training.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  act = env.action_space.sample() \n",
    "  observation, reward, done, info,_ = env.step(act)\n",
    "  # Not printing this time\n",
    "  #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "html = render_mp4(before_training)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(plot_save):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(plot_save)\n",
    "    print(\"The new directory is created!\")\n",
    "for i in range(1000):\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = env.reset()[0]\n",
    "    # print(obs)\n",
    "    while not done:\n",
    "        act = agent.choose_action(obs) \n",
    "        new_state,reward,done,info,_ = env.step(act)\n",
    "        agent.remember(state=obs,action=act,reward=reward,new_state=new_state,done=done)\n",
    "        \n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "    score_history.append(score)\n",
    "    print(f'Episode {i} score {round(score, 2)} 100 game average {round(np.mean(score_history[-100:]), 2)}')\n",
    "    if i % 25 == 0:\n",
    "        agent.save_models()   \n",
    "    filename = f'{i}.png'\n",
    "    plotLearning(score_history,os.path.join(plot_save,filename),window=100)\n",
    "make_gif(plot_save,name_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "before_training = \"before_training.mp4\"\n",
    "\n",
    "video = VideoRecorder(env, before_training)\n",
    "# returns an initial observation\n",
    "env.reset()\n",
    "for i in range(200):\n",
    "  env.render()\n",
    "  video.capture_frame()\n",
    "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
    "  observation, reward, done, info = env.step(env.action_space.sample())\n",
    "  # Not printing this time\n",
    "  #print(\"step\", i, observation, reward, done, info)\n",
    "\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_name in [\n",
    "    'LunarLanderContinuous-v2',\n",
    "    \"BipedalWalker-v3\"]:\n",
    "    env = gym.make(env_name)\n",
    "    input_dims = [*env.observation_space.shape]\n",
    "    n_actions = env.action_space.shape[0]   \n",
    "    \n",
    "    agent = AgentDDPG(\n",
    "    alpha=0.000025,\n",
    "    beta=0.00025,\n",
    "    input_dims=input_dims,\n",
    "    tau=0.001,env=env,\n",
    "    batch_size=64,\n",
    "    layer1_size=400,\n",
    "    layer2_size=300,\n",
    "    n_actions=n_actions,\n",
    "    agent_dir=os.path.join(os.getcwd(),env_name))\n",
    "    \n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    score_history = []\n",
    "    plot_save = os.path.join(os.getcwd(),'plots',env_name)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(plot_save):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(plot_save)\n",
    "        print(\"The new directory is created!\")\n",
    "    for i in range(1000):\n",
    "        done = False\n",
    "        score = 0\n",
    "        obs = env.reset()[0]\n",
    "        # print(obs)\n",
    "        while not done:\n",
    "            act = agent.choose_action(obs) \n",
    "            new_state,reward,done,info,_ = env.step(act)\n",
    "            agent.remember(state=obs,action=act,reward=reward,new_state=new_state,done=done)\n",
    "            agent.learn()\n",
    "            score += reward\n",
    "            obs = new_state\n",
    "        score_history.append(score)\n",
    "        print(f'Episode {i} score {round(score, 2)} 100 game average {round(np.mean(score_history[-100:]), 2)}')\n",
    "        if i % 25 == 0:\n",
    "            agent.save_models()   \n",
    "        filename = f'{i}.png'\n",
    "        plotLearning(score_history,os.path.join(plot_save,filename),window=100)\n",
    "    make_gif(plot_save,env_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
